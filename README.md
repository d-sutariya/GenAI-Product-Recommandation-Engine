# ğŸ¤– GenAI Product Recommendation Engine

A production-ready, **Hexagonal Architecture** (Ports and Adapters) implementation of an Agentic AI system for E-commerce Product Discovery. This system leverages **LangGraph** for orchestration, **Google Gemini & Hugging Face** for reasoning, **Milvus** for vector storage, and the **Model Context Protocol (MCP)** for standardized tool execution.

---

## ğŸ—ï¸ Architecture: Hexagonal & Domain-Driven

This project strictly follows **Hexagonal Architecture** to decouple the core business logic from external tools and frameworks. This ensures testability, maintainability, and the ability to swap infrastructure components (like LLMs or Vector DBs) without touching the core agent logic.

### The Architecture Components

#### ğŸ”µ **Client (Agentic System)**

1.  **ğŸŸ¢ Domain Layer (`client/domain/`)**
    *   **The Core**: Contains pure business logic, data models, and interface definitions (Ports).
    *   **No Dependencies**: This layer *never* imports external infrastructure libraries (like `google.genai` or `milvus`). It only uses Python standard libraries and Pydantic.
    *   **Subdomains**: Structured by bounded contexts:
        *   `perception/`: Logic for understanding user intent
        *   `decision/`: Logic for planning and reasoning
        *   `memory/`: Conversation history and context management
        *   `tools/`: Tool interface definitions
        *   `shared/`: Shared state models
        *   `llm/`: LLM port definitions

2.  **ğŸŸ¡ Application Layer (`client/application/`)**
    *   **The Orchestrator**: Wires the Domain Logic together to perform Use Cases.
    *   **Services**: 
        *   `perception.py`: User intent understanding
        *   `reasoning.py`: Decision-making and planning
        *   `agent_orchestrator.py`: LangGraph workflow orchestration
        *   `client_history_rag.py`: Client conversation history retrieval

3.  **ğŸ”´ Infrastructure Layer (`client/infrastructure/`)**
    *   **The Adapters**: Technical implementations of the Domain Ports.
    *   `llm/`: LLM adapters (Gemini, Hugging Face)
    *   `memory/`: FAISS-based memory adapter
    *   `tools/`: MCP tool adapter for server communication

#### ğŸŸ  **Server (MCP Server & RAG Engine)**

*   **Purpose**: Standalone FastMCP server providing product search and metadata tools
*   **Components**:
    *   `services/`: Milvus, embedding, and ingestion services
    *   `tools/`: Product search and ranking tools exposed via MCP
    *   `models/`: Product data models
    *   `config/`: Server configuration and settings

#### ğŸŸ£ **Infrastructure & Deployment**

*   `k8s/`: Kubernetes manifests with ArgoCD configuration
*   `tests/`: Unit and E2E tests
*   `pyproject.toml`: Project dependencies and metadata

---

## ğŸ“‚ Project Structure

```text
GenAI-Product-Recommandation-Engine/
â”œâ”€â”€ pyproject.toml              # Project configuration & dependencies
â”œâ”€â”€ README.md                   # This file
â”‚
â”œâ”€â”€ client/                     # ğŸ”µ AGENTIC CLIENT
â”‚   â”œâ”€â”€ main.py                 # Entry point (Dependency Injection)
â”‚   â”œâ”€â”€ Dockerfile              # Client containerization
â”‚   â”œâ”€â”€ requirements.txt        # Client dependencies
â”‚   â”œâ”€â”€ domain/                 # CORE: Models & Ports (Interfaces)
â”‚   â”‚   â”œâ”€â”€ decision/           # Decision-making models
â”‚   â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ llm/                # LLM port definitions
â”‚   â”‚   â”‚   â””â”€â”€ llm_port.py
â”‚   â”‚   â”œâ”€â”€ memory/             # Memory interfaces
â”‚   â”‚   â”‚   â”œâ”€â”€ memory_port.py
â”‚   â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ perception/         # Intent understanding models
â”‚   â”‚   â”‚   â””â”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ shared/             # Shared state
â”‚   â”‚   â”‚   â””â”€â”€ state.py
â”‚   â”‚   â””â”€â”€ tools/              # Tool definitions
â”‚   â”‚       â”œâ”€â”€ models.py
â”‚   â”‚       â””â”€â”€ tool_port.py
â”‚   â”œâ”€â”€ application/            # ORCHESTRATION: Business Logic
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ agent_orchestrator.py    # LangGraph workflow
â”‚   â”‚       â”œâ”€â”€ client_history_rag.py    # Conversation retrieval
â”‚   â”‚       â”œâ”€â”€ perception.py            # Intent analysis
â”‚   â”‚       â””â”€â”€ reasoning.py             # Decision service
â”‚   â”œâ”€â”€ infrastructure/         # ADAPTERS: External integrations
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini_adapter.py        # Google Gemini adapter
â”‚   â”‚   â”‚   â””â”€â”€ huggingface_adapter.py   # HuggingFace adapter
â”‚   â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”‚   â””â”€â”€ faiss_memory_adapter.py  # FAISS memory store
â”‚   â”‚   â””â”€â”€ tools/
â”‚   â”‚       â””â”€â”€ mcp_tool_adapter.py      # MCP client adapter
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py           # Logging utilities
â”‚
â”œâ”€â”€ server/                     # ğŸŸ  MCP SERVER & RAG ENGINE
â”‚   â”œâ”€â”€ main.py                 # FastMCP server entry point
â”‚   â”œâ”€â”€ pipeline.py             # Data processing pipeline
â”‚   â”œâ”€â”€ Dockerfile              # Server containerization
â”‚   â”œâ”€â”€ requirements.txt        # Server dependencies
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py         # Server configuration (Milvus, etc.)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ products.py         # Product data models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ embedding_service.py    # Text embedding generation
â”‚   â”‚   â”œâ”€â”€ ingestion_service.py    # Data ingestion logic
â”‚   â”‚   â””â”€â”€ milvus_service.py       # Milvus vector DB operations
â”‚   â”œâ”€â”€ tools/
â”‚   â”‚   â””â”€â”€ product_tools.py    # MCP tools (search, rank, analyze)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py           # Server logging
â”‚
â”œâ”€â”€ k8s/                        # â˜¸ï¸ KUBERNETES DEPLOYMENT
â”‚   â”œâ”€â”€ argocd-app.yaml         # ArgoCD application definition
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ configmap.yaml      # ConfigMaps
â”‚   â”‚   â”œâ”€â”€ namespace.yaml      # Namespace definition
â”‚   â”‚   â””â”€â”€ secrets.yaml        # Secrets (API keys, etc.)
â”‚   â”œâ”€â”€ client/
â”‚   â”‚   â””â”€â”€ deployment.yaml     # Client deployment
â”‚   â””â”€â”€ server/
â”‚       â”œâ”€â”€ deployment.yaml     # Server deployment
â”‚       â””â”€â”€ service.yaml        # Server service
â”‚
â””â”€â”€ tests/                      # ğŸ§ª TESTS
    â”œâ”€â”€ conftest.py             # Pytest configuration
    â”œâ”€â”€ e2e/
    â”‚   â””â”€â”€ test_workflow.py    # End-to-end workflow tests
    â””â”€â”€ unit/
        â”œâ”€â”€ application/        # Application layer tests
        â”‚   â”œâ”€â”€ test_perception_service.py
        â”‚   â””â”€â”€ test_reasoning_service.py
        â”œâ”€â”€ domain/             # Domain model tests
        â”‚   â””â”€â”€ test_models.py
        â””â”€â”€ infrastructure/     # Infrastructure adapter tests
            â”œâ”€â”€ test_llm_adapter.py
            â”œâ”€â”€ test_memory_adapter.py
            â””â”€â”€ test_tool_adapter.py
```

---

## ğŸš€ Key Features

### ğŸ¤– **Agentic AI System**
- **Cognitive Cycle**: Implements a `Perceive â†’ Remember â†’ Decide â†’ Act` loop using LangGraph
- **Multi-LLM Support**: Seamlessly switch between Google Gemini and Hugging Face models
- **Structured Reasoning**: Uses Pydantic models to enforce structured output, reducing hallucinations
- **Conversation Memory**: FAISS-based memory for context-aware interactions

### ğŸ”§ **Model Context Protocol (MCP)**
- **Standardized Tool Execution**: All product search tools exposed via MCP standard
- **Microservices Architecture**: Separate client (agent) and server (tools) components
- **FastMCP Framework**: Modern, fast MCP server implementation

### ğŸ” **Advanced RAG Engine**
- **Vector Search**: Milvus Lite for high-performance vector similarity search
- **Smart Embedding**: Google's text-embedding-004 model (768 dimensions)
- **Product Ranking**: Multi-stage ranking and refinement tools
- **Metadata Analysis**: Advanced filtering and re-ranking capabilities

### â˜ï¸ **Cloud-Native Deployment**
- **Kubernetes Ready**: Complete K8s manifests with ArgoCD GitOps
- **Containerized**: Docker support for both client and server
- **Scalable**: Microservices architecture for independent scaling
- **Observable**: Prometheus metrics integration

### ğŸ§ª **Production Quality**
- **Hexagonal Architecture**: Clean separation of concerns
- **Comprehensive Testing**: Unit and E2E test coverage
- **Type Safety**: Full Pydantic model validation
- **Logging**: Structured logging throughout

---

## ğŸ› ï¸ Setup & Usage

### Prerequisites
- Python 3.10 (specifically, not 3.11+)
- Google Gemini API Key (or Hugging Face token)
- Docker (optional, for containerized deployment)
- Kubernetes cluster (optional, for production deployment)

### Local Development Setup

#### 1. Clone the Repository
```bash
git clone https://github.com/d-sutariya/GenAI-Product-Recommandation-Engine.git
cd GenAI-Product-Recommandation-Engine
```

#### 2. Environment Configuration
Create a `.env` file in the root directory:
```ini
# Required: LLM API Key
GEMINI_API_KEY=your_gemini_api_key_here

# Optional: MCP Server URL (if running separately)
MCP_SERVER_URL=http://localhost:8000/sse

# Optional: HuggingFace Token (if using HF models)
HUGGINGFACE_TOKEN=your_hf_token_here
```

#### 3. Install Dependencies

**For Client:**
```bash
cd client
pip install -r requirements.txt
```

**For Server:**
```bash
cd server
pip install -r requirements.txt
```

**Or use the project-wide dependencies:**
```bash
# From project root
pip install -e .
```

#### 4. Run the MCP Server

The server handles product search and RAG operations:

```bash
cd server
python main.py
```

The server will:
- Initialize Milvus Lite vector database
- Ingest product data (if not already done)
- Start FastMCP server on `http://localhost:8000`

#### 5. Run the Client Agent

In a new terminal, start the agent:

```bash
cd client
python main.py
```

The agent will:
- Connect to the MCP server
- Start an interactive conversation loop
- Process your product queries using LangGraph workflow

### Example Queries

```text
"Find me Nike running shoes under $100"
"Show me waterproof hiking backpacks"
"I need a formal watch for business meetings"
"Looking for casual summer dresses"
```

---

## ğŸ³ Docker Deployment

### Build Images

**Server:**
```bash
cd server
docker build -t product-recommendation-server:latest .
```

**Client:**
```bash
cd client
docker build -t product-recommendation-client:latest .
```

### Run with Docker

**Start Server:**
```bash
docker run -p 8000:8000 \
  -e GEMINI_API_KEY=your_key \
  product-recommendation-server:latest
```

**Start Client:**
```bash
docker run -it \
  -e GEMINI_API_KEY=your_key \
  -e MCP_SERVER_URL=http://server:8000/sse \
  product-recommendation-client:latest
```

---

## â˜¸ï¸ Kubernetes Deployment

### Using ArgoCD (Recommended)

1. **Install ArgoCD** in your cluster
2. **Apply the ArgoCD Application:**
   ```bash
   kubectl apply -f k8s/argocd-app.yaml
   ```

3. **Configure Secrets:**
   ```bash
   kubectl create secret generic app-secrets \
     --from-literal=GEMINI_API_KEY=your_key \
     -n product-recommendation
   ```

### Manual Kubernetes Deployment

```bash
# Create namespace and common resources
kubectl apply -f k8s/common/

# Deploy server
kubectl apply -f k8s/server/

# Deploy client
kubectl apply -f k8s/client/
```

---

## ğŸ§  Cognitive Flow (How It Works)

### High-Level Architecture

```mermaid
graph TB
    subgraph USER["ğŸ‘¤ User"]
        UI["User Query<br/>(CLI / stdin)"]
    end

    subgraph CLIENT["ğŸ–¥ï¸ Client â€” Hexagonal Architecture"]
        MAIN_C["client/main.py"]

        subgraph INFRA["Infrastructure Adapters"]
            LLM_A["HFLLMAdapter / GeminiLLMAdapter"]
            MEM_A["FaissMemoryAdapter"]
            TOOL_A["MCPToolAdapter"]
        end

        subgraph APP["Application Services"]
            PERCEPT["PerceptionService"]
            DECISION["DecisionService"]
            ORCH["AgentWorkflow<br/>(LangGraph StateGraph)"]
            RAG_C["ClientHistoryRAGService"]
        end

        subgraph DOMAIN["Domain Layer â€” Ports & Models"]
            LLM_P["LLMProvider Port"]
            MEM_P["MemoryStore Port"]
            TOOL_P["ToolExecutor Port"]
            STATE["AgentState"]
            MODELS["PerceptionResult / DecisionResult / MemoryRecord"]
        end
    end

    subgraph SERVER["âš™ï¸ MCP Server â€” FastMCP"]
        MAIN_S["server/main.py<br/>(stdio / SSE)"]

        subgraph MCP_TOOLS["Registered MCP Tools"]
            T1["search_products"]
            T2["format_product_metadata"]
            T3["rerank_products"]
            T4["get_product_attributes"]
        end

        subgraph SERVICES["Server Services"]
            INGEST["IngestionService"]
            EMBED["EmbeddingService<br/>(Google Gemini)"]
            MILVUS["MilvusService<br/>(Milvus Lite)"]
        end
    end

    subgraph EXTERNAL["â˜ï¸ External Services"]
        GEMINI_API["Google Gemini API"]
        MILVUS_DB["Milvus Lite DB<br/>(products.db)"]
        PROM["Prometheus Metrics<br/>(:8000)"]
    end

    subgraph DATA["ğŸ“‚ Data"]
        JSON["Product JSON Files<br/>(documents/*.json)"]
    end

    UI --> MAIN_C
    MAIN_C --> LLM_A
    MAIN_C --> MEM_A
    MAIN_C --> TOOL_A
    MAIN_C --> ORCH

    LLM_A -.->|implements| LLM_P
    MEM_A -.->|implements| MEM_P
    TOOL_A -.->|implements| TOOL_P

    ORCH --> PERCEPT
    ORCH --> DECISION
    PERCEPT --> LLM_P
    DECISION --> LLM_P

    TOOL_A ==>|"MCP Protocol<br/>(stdio / SSE)"| MAIN_S

    MAIN_S --> T1
    MAIN_S --> T2
    MAIN_S --> T3
    MAIN_S --> T4

    T1 --> EMBED
    T1 --> MILVUS
    INGEST --> EMBED
    INGEST --> MILVUS

    EMBED --> GEMINI_API
    MILVUS --> MILVUS_DB
    MAIN_S --> PROM
    JSON --> INGEST

    MAIN_C --> RAG_C
    RAG_C --> MEM_A
```

### Agent Workflow â€” LangGraph State Machine

```mermaid
stateDiagram-v2
    [*] --> Perception

    Perception --> Memory: Extract intent & entities via LLM
    Memory --> Decision: Retrieve relevant memories (FAISS)

    Decision --> MCP_Tool_Execution: decision_type = "tool_call"
    Decision --> Add_To_Cart: decision_type = "final_answer" + recommended_product
    Decision --> End: decision_type = "final_answer" (no product)

    MCP_Tool_Execution --> Memory_Update: Store tool output in memory

    Memory_Update --> Decision: step < max_steps (loop)
    Memory_Update --> End: final_answer found
    Memory_Update --> Error_Handler: error occurred

    Add_To_Cart --> End
    Error_Handler --> End

    End --> [*]
```

### Data Ingestion Pipeline

```mermaid
flowchart LR
    A["ğŸ“‚ Product JSON Files"] --> B["IngestionService"]
    B --> C{"File changed?<br/>(MD5 cache check)"}
    C -->|Yes| D["Parse JSON â†’ ProductChunkTyped"]
    C -->|No| E["Skip"]
    D --> F["EmbeddingService<br/>(Gemini API)"]
    F --> G["Generate Vector Embedding"]
    G --> H["MilvusService.insert_data()"]
    H --> I["ğŸ—„ï¸ Milvus Lite DB"]
    B --> J["Update ingestion_cache.json"]
```

### Runtime Search / RAG Sequence

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ User
    participant C as Client Agent
    participant LLM as LLM (HuggingFace/Gemini)
    participant MCP as MCP Server
    participant EMB as Embedding Service
    participant DB as Milvus DB

    U->>C: "Show me high performance laptops"
    C->>LLM: Perception â€” extract intent & entities
    LLM-->>C: {intent: product_search, entities: [laptop, high performance]}
    C->>C: Retrieve FAISS memories
    C->>LLM: Decision â€” what to do next?
    LLM-->>C: {tool_call: search_products, args: {query: "high performance laptop"}}
    C->>MCP: search_products("high performance laptop", top_k=5)
    MCP->>EMB: get_embedding("high performance laptop")
    EMB->>EMB: Gemini API â†’ 768-dim vector
    EMB-->>MCP: embedding vector
    MCP->>DB: vector similarity search
    DB-->>MCP: Top 5 product results
    MCP-->>C: ProductResponse list
    C->>C: Store tool output in memory
    C->>LLM: Decision â€” enough info?
    LLM-->>C: {final_answer: "Here are the top laptops...", recommended_product: "Dell XPS 15"}
    C->>U: Display results + "Add to cart?" prompt
    C->>C: Save interaction to Client History RAG (FAISS)
```

### Detailed Flow

1. **Perception Service** â†’ Analyzes user intent, extracts entities (brand, category, price, etc.)
2. **Memory Service** â†’ Retrieves conversation history and user preferences from FAISS
3. **Decision Service** â†’ LLM decides what action to take (search, clarify, answer)
4. **MCP Tool Execution** â†’ Calls product search tools on the server
5. **RAG Engine** â†’ Milvus vector search finds semantically similar products
6. **Ranking & Refinement** â†’ Advanced filtering based on user criteria
7. **Response Generation** â†’ Natural language response with product recommendations

---

## ğŸ§ª Testing

### Run Unit Tests
```bash
pytest tests/unit/ -v
```

### Run E2E Tests
```bash
pytest tests/e2e/ -v
```

### Run All Tests with Coverage
```bash
pytest tests/ -v --cov=client --cov=server --cov-report=html
```

---

## ğŸ”„ Architecture Benefits

### Why Hexagonal Architecture?

âœ… **Testability**: Domain logic can be tested without external dependencies  
âœ… **Flexibility**: Swap LLMs (Gemini â†” HuggingFace) or vector DBs (Milvus â†” FAISS) easily  
âœ… **Maintainability**: Clear boundaries between business logic and infrastructure  
âœ… **Scalability**: Independent scaling of client and server components  

### Why MCP?

âœ… **Standardization**: Industry-standard protocol for AI tool integration  
âœ… **Interoperability**: Works with any MCP-compatible client  
âœ… **Extensibility**: Easy to add new tools without changing agent code  
âœ… **Separation**: Clean separation between reasoning and tool execution  

---

## ğŸ“Š Tech Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Orchestration** | LangGraph | Agent workflow state machine |
| **LLM** | Google Gemini, HuggingFace | Reasoning and language understanding |
| **Vector DB** | Milvus Lite | Product embeddings storage |
| **Embeddings** | text-embedding-004 | Text to vector conversion |
| **Memory** | FAISS | Conversation history |
| **Protocol** | FastMCP | Tool execution standard |
| **Framework** | FastAPI | HTTP server (MCP) |
| **Container** | Docker | Application containerization |
| **Orchestration** | Kubernetes | Production deployment |
| **GitOps** | ArgoCD | Continuous deployment |
| **Testing** | Pytest | Unit and E2E tests |
| **Validation** | Pydantic | Type safety and validation |

---

## ğŸ“ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ™ Acknowledgments

- **Anthropic** for the Model Context Protocol specification
- **Google** for Gemini API and embedding models
- **Hugging Face** for open-source models and ecosystem
- **Milvus** for vector database technology
- **LangChain/LangGraph** for agent orchestration framework

---

## ğŸ“§ Contact

**Deep Sutariya**
- GitHub: [@d-sutariya](https://github.com/d-sutariya)
- Project Link: [GenAI-Product-Recommandation-Engine](https://github.com/d-sutariya/GenAI-Product-Recommandation-Engine)

---

**â­ If you find this project useful, please consider giving it a star!**
